---
layout: post
title: Lessons Learned (Part 1: Remembering Waldo)
date: 2009-10-29
comments: false
---

<h1>{{ page.title }}</h1>
<div class='post'>
<span style="font-weight: bold;">Intro</span><br /><br />I've spent the last couple of months trying to help improve performance and scalability of a large web-based system. Initially, the application could barely handle more than a handful of concurrent users, which was far away from the launch target of several million users per day. This will probably be the first in a series of posts, in which I'd like to talk about some of the more interesting challenges we've faced.<br /><br />A bit of context first: The application was written in C#. The main technologies involved were WCF, MSMQ and SQLServer. Roughly speaking, the app consisted of a presentation tier (IIS/MVC), a business logic tier (WCF) and a data tier (SQLServer). Application logic was exposed through a large number of WCF service endpoints. Each service endpoint, in turn, exposed a similarly large number of fairly fine-grained service operations. Essentially, there were three groups of clients that consumed the exposed services: the presentation tier, other WCF services inside the business logic tier and a number of mobile clients (which I won't talk about here).<br /><br /><span style="font-weight: bold;">Intra-Tier Communication and Horizontal Scaling</span><br /><br />Inside the business logic tier, there was quite a lot of communication going on between the individual WCF services. Initially, many of these calls were routed through the WCF stack. The rationale behind this initial design decision was so that - if needed at a later stage - some services could be run individually on separate machines.<br /><br />It seemed unlikely, however, that this would ever happen. The logical conclusion of this thought was that services would communicate with each other via network calls, even though they would be running inside the same process. And even if the WCF service layer would be partitioned and distributed onto separate machines, what would happen if this still wouldn't give us the desired performance? Imagine you've got three WCF services: s1, s2 and s3. Assuming that s3 is the most hardware-hungry one, we could deploy s1 and s2 together on one machine and s3 separately on a dedicated machine. What happens, though, if the hardware onto which s3 is running is still insufficient? At this point, we could <a href="http://news.zdnet.com/2422-13569_22-155070.html" target="_blank">start scaling the service out horizontally</a> by adding a load balancer and more machines each running a copy of s3. So, if we probably need to scale out horizontally anyway at some stage, what's the point of adding the overhead and complexity of network calls between services if they can be run in same address space? To emphasise this point, we measured how many WCF service calls we can make in a given period (using net:TCP binding) and compared this against making direct in-memory calls to the same service instance. The not unexpected result: throughput for the latter was about 350 times higher. Consequently, we went through the codebase and replaced WCF service calls with normal method invocations wherever possible.<br /><br />In a recent email conversation, my colleague <a href="http://www.martinfowler.com/" target="_blank">Martin Fowler</a> drew an interesting analogy: <span style="font-style: italic;">"It's interesting that there continues to be this desire to distribute different functionality onto different nodes in the name of scalability when often the better route is to put all nodes in the same process and cluster the resulting app. This was exactly the wrong thought that distributed objects suffered from."</span><br /><br /><span style="font-weight: bold;">Inter-Tier Communication and Horizontal Scaling</span><br /><br />The presentation tier was physically separated from the WCF services running in the business logic tier. Consequently, communication between the two tiers had to happened over the network.<br /><br />Let's go back in time a little. Back in 1994, Waldo et al. wrote their excellent seminal paper called "<a href="http://research.sun.com/techrep/1994/smli_tr-94-29.pdf" target="_blank">A Note on Distributed Computing</a>". In it, they argue that there are fundamental differences between in-process and intra-process calls in terms of latency, concurrency, partial failures scenarios etc. In the past, RPC systems have tried to abstract these differences away and make developers believe that there's no difference between calling an object in the same memory space or executing a procedure on a remote machine.<br /><br />Don't get me wrong, I think that WCF is actually a pretty cool platform but, unfortunately, it also encourages people to continue building RPC apps in cases where other solutions might be more favorable. In fact, it makes it horribly easy to take a bunch of classes and expose them as remote objects. Calling them services doesn't mean that your application has now magically become service-oriented. Also, it doesn't change the fact that these now-called services are still remote objects including all the flaws that Waldo talked about.<br /><br />Indeed, we've had to fix a lot of code where developers happily looped over hundreds or thousands of items in order to retrieve some data, unaware that in each iteration they were making a network call. It goes without saying that this had a significant impact on the performance of the system. After removing all unnecessary calls, we measured the overhead incurred by network communication again. On average, still more than 30% of our total service execution time was network overhead (i.e. serialisation, WCF, TCP, network latency...).<br /><br />Looking at the chatty WCF service interfaces and the tight coupling with the code in the presentation tier it occurred to me that, in actual fact, we were not really building a distributed application but we were distributing an essentially monolithic application. Unfortunately, we didn't actually manage to change this one. I'm convinced, though, that a better way is to deploy two tightly coupled tiers together in the same process and then, again, <a href="http://news.zdnet.com/2422-13569_22-155070.html" target="_blank">scale out horizontally</a>.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>praful todkar</div>
<div class='content'>
Nice post. But I guess, in your case, you would have to worry about the mobile clients who are consuming the business services before merging the two tiers. It is a tough science to choose the granularity of your services  especially if you have clients that expect data at different granularities. But I agree, do not distribute tightly coupled operations.</div>
</div>
<div class='comment'>
<div class='author'>Patric Fornasier</div>
<div class='content'>
@toast *hehe* was hoping to drive some additional traffic to the post ;)</div>
</div>
<div class='comment'>
<div class='author'>toast</div>
<div class='content'>
hey patric,<br />I see you managed to get a plug in for Martin :)<br />All this network pain reminds me too much of EJBs, which also worked in the opposite direction to Waldo.  Why do we do these things to ourselves over and over.<br />I find this sort of thing very Agile promoting;  you ain&#39;t gonna need it.  (Until Patric proves you do because he actually performance tested the app...)</div>
</div>
<div class='comment'>
<div class='author'>iansrobinson</div>
<div class='content'>
Good post.<br /><br />Three-layered application architecture at distributed propertions. D&#39;oh! <br /><br />Sadly, the orthodox model for &quot;service-oriented&quot; design.</div>
</div>
<div class='comment'>
<div class='author'>Jim Barritt</div>
<div class='content'>
Hi Pat,<br /><br />thanks for that link about distributed computing article, I was actually talking about that the other day, as you do, and couldnt remember which one it was.<br /><br />Its interesting that on a recent project I also encountered the belief that in order to scale, components would need to be distributed over many nodes. Fortunately I was able to rip it apart and stick it back together in a single web server resulting in order of 10 magnitude increase in throughput.<br /><br />I seem to remember something about Rule #1 of distributed computing being &quot;don&#39;t distribute&quot; for exactly the reasons you mention here.</div>
</div>
<div class='comment'>
<div class='author'>Udi</div>
<div class='content'>
Great post - I&#39;ve had many of same kinds of experiences. These have led me to come out with a one-way messaging infrastructure that makes it difficult for developers to make these sorts of mistakes:<br /><br /><a href="http://www.NServiceBus.com" rel="nofollow">NServiceBus</a><br /><br />I&#39;d be interested in hearing your impressions.</div>
</div>
</div>
